{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22de5f2e",
   "metadata": {},
   "source": [
    "# ⚡️ Designing Market Shock Scenarios Notebook (Abdymomunov et al. (2024))\n",
    "[Paper Link](https://www.richmondfed.org/-/media/RichmondFedOrg/publications/research/working_papers/2024/wp24-17.pdf)\n",
    "\n",
    "This notebook re-implements the framework from Abdymomunov et al. (2024) using simulated data:\n",
    "1. Two-stage modelling of risk-factor relationships\n",
    "\n",
    "2. Quantile regressions (primary → secondary)\n",
    "\n",
    "3. t-copula with marginal GARCH (secondary → all remaining)\n",
    "\n",
    "4. Scenario generation & selection\n",
    "\n",
    "4. Historical simulation of primary factors\n",
    "\n",
    "5. Material-risk-factor screening (PnL coverage)\n",
    "\n",
    "6. Tail-loss filter (1st percentile)\n",
    "\n",
    "7. K-means reduction to a small representative set\n",
    "\n",
    "When real market or portfolio data are unavailable the notebook fabricates synthetic but statistically plausible data so every block runs out-of-the-box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from arch import arch_model\n",
    "from scipy.stats import t\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Helper functions\n",
    "def simulate_ou(mu, theta, sigma, n):\n",
    "    \"\"\"Simulate Ornstein-Uhlenbeck process for interest rate factors\"\"\"\n",
    "    x = np.zeros(n); x[0] = mu\n",
    "    for i in range(1, n):\n",
    "        x[i] = x[i-1] + theta*(mu - x[i-1]) + sigma*np.random.randn()\n",
    "    return x\n",
    "\n",
    "def fix_corr_matrix(corr):\n",
    "    \"\"\"Fix correlation matrix to be positive semi-definite\"\"\"\n",
    "    corr = (corr + corr.T)/2\n",
    "    eigvals, eigvecs = np.linalg.eigh(corr)\n",
    "    eigvals[eigvals < 1e-6] = 1e-6\n",
    "    corr_fixed = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "    d = np.sqrt(np.diag(corr_fixed))\n",
    "    corr_fixed = corr_fixed / np.outer(d, d)\n",
    "    np.fill_diagonal(corr_fixed, 1.0)\n",
    "    return corr_fixed\n",
    "\n",
    "def get_tau(hist, x0):\n",
    "    \"\"\"Calculate tau based on historical severity of shock (Equation 3)\"\"\"\n",
    "    p = np.mean(np.abs(hist) >= abs(x0))\n",
    "    tau = min(max(round(p/0.05)*0.05, 0.10), 0.90)\n",
    "    return tau\n",
    "\n",
    "def quantile_predict_shocks(Xh, yh, xs):\n",
    "    \"\"\"Standard quantile regression for secondary factors (Equation 1)\"\"\"\n",
    "    preds = []\n",
    "    for x0 in xs:\n",
    "        tau = get_tau(Xh, x0)\n",
    "        qr = QuantileRegressor(quantile=tau, alpha=0)\n",
    "        qr.fit(Xh.reshape(-1,1), yh)\n",
    "        preds.append(qr.predict([[x0]])[0])\n",
    "    return np.array(preds)\n",
    "\n",
    "def quantile_autoreg_predict(Xh, yh, xs, horizon=3):\n",
    "    \"\"\"\n",
    "    Quantile autoregression for factors with strong autocorrelation (Equation 2)\n",
    "    - yh: factor levels (not shocks)\n",
    "    - Xh: primary factor shocks\n",
    "    - xs: new primary factor shocks to predict for\n",
    "    - horizon: shock calibration horizon (e.g., 3 months)\n",
    "    \"\"\"\n",
    "    # Estimate quantile autoregression model\n",
    "    tau = get_tau(Xh, xs[0])  # Use first shock to determine tau\n",
    "    \n",
    "    # Prepare data for autoregression\n",
    "    y_lagged = yh[:-1]  # Y_{t-1}\n",
    "    X = Xh[1:]         # X_{Δt}\n",
    "    y = yh[1:]         # Y_t\n",
    "    \n",
    "    # Fit quantile regression\n",
    "    qr = QuantileRegressor(quantile=tau, alpha=0)\n",
    "    qr.fit(np.column_stack([X, y_lagged]), y)\n",
    "    \n",
    "    # Predict shocks recursively\n",
    "    preds = []\n",
    "    for x0 in xs:\n",
    "        # Start with current level\n",
    "        y0 = yh[-1]\n",
    "        y_pred = y0\n",
    "        \n",
    "        # Recursively predict h months ahead\n",
    "        for _ in range(horizon):\n",
    "            y_pred = qr.predict([[x0, y_pred]])[0]\n",
    "        \n",
    "        # Shock = Y_h - Y_0\n",
    "        preds.append(y_pred - y0)\n",
    "    \n",
    "    return np.array(preds)\n",
    "\n",
    "def safe_garch_fit(returns):\n",
    "    \"\"\"Apply GARCH filtering for marginal distributions\"\"\"\n",
    "    try:\n",
    "        m = arch_model(returns, vol='Garch', p=1, q=1)\n",
    "        r = m.fit(disp='off')\n",
    "        if r.converged:\n",
    "            return r.resid\n",
    "    except:\n",
    "        pass\n",
    "    return (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "def filter_implausible_scenarios(primary_shocks, primary_levels):\n",
    "    \"\"\"\n",
    "    Filter out implausible scenarios (e.g., negative interest rates)\n",
    "    Paper: \"filter out historical realizations of the U.S. Treasury bond yield shocks \n",
    "    that cause post-shock rate levels to be negative\"\n",
    "    \"\"\"\n",
    "    # Check if post-shock rates would be negative\n",
    "    plausible_mask = np.ones(len(primary_shocks), dtype=bool)\n",
    "    \n",
    "    # For interest rate factors (UST10, TERM, UST3)\n",
    "    rate_factors = ['UST10', 'TERM', 'UST3']\n",
    "    for factor in rate_factors:\n",
    "        if factor in primary_levels.columns:\n",
    "            # Current level + shock <= 0 would be implausible\n",
    "            implausible = (primary_levels[factor].iloc[-1] + primary_shocks[factor] <= 0)\n",
    "            plausible_mask &= ~implausible.values\n",
    "    \n",
    "    return primary_shocks[plausible_mask]\n",
    "\n",
    "# 1. Simulate primary factors with realistic interest rate dynamics\n",
    "dates = pd.date_range(\"1990-01-01\", \"2025-06-30\", freq=\"M\")\n",
    "n = len(dates)\n",
    "\n",
    "# Simulate interest rate factors with realistic mean-reversion\n",
    "ust10 = simulate_ou(0.025, 0.05, 0.005, n)  # 10-year Treasury yield\n",
    "term = simulate_ou(0.005, 0.1, 0.003, n)    # Term spread\n",
    "ust3 = ust10 - term                        # 3-month Treasury yield\n",
    "\n",
    "# Ensure interest rates stay positive\n",
    "ust10 = np.maximum(ust10, 0.001)\n",
    "ust3 = np.maximum(ust3, 0.001)\n",
    "\n",
    "primary = pd.DataFrame({\n",
    "    \"SP500\": simulate_ou(0.002, 0.1, 0.05, n),\n",
    "    \"BAA_AAA\": simulate_ou(0.02, 0.2, 0.01, n),\n",
    "    \"UST10\": ust10,\n",
    "    \"TERM\": term,\n",
    "    \"USD_EUR\": simulate_ou(0, 0.1, 0.03, n),\n",
    "    \"ENERGY\": simulate_ou(0.001, 0.1, 0.04, n),\n",
    "    \"METAL\": simulate_ou(0.001, 0.1, 0.035, n),\n",
    "    \"GOLD\": simulate_ou(0.001, 0.15, 0.06, n)\n",
    "}, index=dates)\n",
    "\n",
    "# Add 3-month Treasury yield\n",
    "primary[\"UST3\"] = ust3\n",
    "\n",
    "# Calculate rolling 3-month shocks (paper: \"rolling-window of three-month changes\")\n",
    "primary_shocks = pd.DataFrame(index=primary.index[3:], columns=primary.columns)\n",
    "for col in primary.columns:\n",
    "    primary_shocks[col] = primary[col].values[3:] - primary[col].values[:-3]\n",
    "\n",
    "# Filter out implausible scenarios (e.g., negative interest rates)\n",
    "primary_levels = primary.iloc[3:]  # Current levels corresponding to shocks\n",
    "primary_shocks = filter_implausible_scenarios(primary_shocks, primary_levels)\n",
    "\n",
    "# Display primary shocks\n",
    "display(primary_shocks.head().style.format(\"{:.5f}\").set_caption(\"Primary Factor 3-Month Shocks Sample\"))\n",
    "\n",
    "# 2. Simulate secondary factors with appropriate modeling choices\n",
    "n_sec = 100\n",
    "sec_names = [f\"SEC{i+1}\" for i in range(n_sec)]\n",
    "secondary = pd.DataFrame(index=primary_shocks.index, columns=sec_names)\n",
    "\n",
    "# Create secondary factors with different characteristics\n",
    "for i, name in enumerate(sec_names):\n",
    "    noise = np.random.normal(0, 0.02, len(primary_shocks))\n",
    "    \n",
    "    # Some secondary factors have strong autocorrelation (like volatility)\n",
    "    if i % 10 == 0:  # Every 10th factor has strong autocorrelation\n",
    "        # Simulate as levels with autocorrelation\n",
    "        secondary[name] = 0.8 * secondary[name].shift(1).fillna(0.0) + 0.6 * primary_shocks[\"SP500\"] + 0.4 * primary_shocks[\"TERM\"] + noise\n",
    "    else:\n",
    "        # Standard secondary factors\n",
    "        secondary[name] = 0.6 * primary_shocks[\"SP500\"] + 0.4 * primary_shocks[\"TERM\"] + noise\n",
    "\n",
    "secondary_sim = pd.DataFrame(index=primary_shocks.index, columns=sec_names)\n",
    "Xh = primary_shocks[\"SP500\"].values\n",
    "\n",
    "# Use appropriate model for each secondary factor\n",
    "for i, name in enumerate(sec_names):\n",
    "    if i % 10 == 0:  # Every 10th factor uses quantile autoregression\n",
    "        # For autoregressive factors, we need the levels (not shocks)\n",
    "        yh_levels = secondary[name].cumsum()  # Convert shocks to levels\n",
    "        secondary_sim[name] = quantile_autoreg_predict(Xh, yh_levels.values, Xh)\n",
    "    else:\n",
    "        secondary_sim[name] = quantile_predict_shocks(Xh, secondary[name].values, Xh)\n",
    "\n",
    "display(secondary_sim.iloc[:5,:5].describe().style.set_caption(\"Sample Secondary Factor Shocks Summary\"))\n",
    "\n",
    "# 3. Simulate remaining factors and build t-copula\n",
    "n_rem = 500\n",
    "rem_names = [f\"REM{i+1}\" for i in range(n_rem)]\n",
    "rem_data = pd.DataFrame({name: safe_garch_fit(np.random.normal(0,0.01,n)) for name in rem_names}, index=dates)\n",
    "rem_data = rem_data.loc[primary_shocks.index]\n",
    "pits = rem_data.rank(pct=True).values\n",
    "pits_ann = pd.DataFrame(pits, index=primary_shocks.index).groupby(primary_shocks.index.year).last()\n",
    "df_t = 4\n",
    "tq = t.ppf(np.clip(pits_ann.values,1e-6,1-1e-6),df=df_t)\n",
    "rho = fix_corr_matrix(np.corrcoef(tq.T))\n",
    "\n",
    "# 4. Joint scenario simulation\n",
    "N = 1000\n",
    "# Use rolling-window historical simulation approach from paper\n",
    "idx = np.random.choice(len(primary_shocks)-30, N, replace=True)  # Avoid edge cases\n",
    "idx = idx + 15  # Center the window\n",
    "pri_sim = primary_shocks.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "sec_sim = secondary_sim.iloc[idx].reset_index(drop=True)\n",
    "zs = np.random.standard_t(df=df_t, size=(N,n_rem))\n",
    "L = np.linalg.cholesky(rho)\n",
    "rem_sim = (zs @ L.T) * rem_data.std().values\n",
    "scenarios = np.hstack([pri_sim.values, sec_sim.values, rem_sim])\n",
    "all_names = list(primary_shocks.columns) + sec_names + rem_names\n",
    "\n",
    "# 5. Synthetic PnL sensitivities & distributions\n",
    "dv01 = pd.DataFrame(np.random.uniform(-10000,20000,(2,len(all_names))),\n",
    "                    index=[\"Firm_A\",\"Firm_B\"], columns=all_names)\n",
    "conv = pd.DataFrame(np.random.uniform(-50,50,(2,len(all_names))),\n",
    "                    index=[\"Firm_A\",\"Firm_B\"], columns=all_names)\n",
    "def pnl(f,sh):\n",
    "    bps = sh*10000\n",
    "    return -(dv01.loc[f].values*bps).sum() + 0.5*(conv.loc[f].values*(bps**2)).sum()\n",
    "\n",
    "# === PnL Calculation Section ===\n",
    "pnlA = np.array([pnl(\"Firm_A\",scenarios[i]) for i in range(N)])/1e6\n",
    "pnlB = np.array([pnl(\"Firm_B\",scenarios[i]) for i in range(N)])/1e6\n",
    "\n",
    "# PnL summary table\n",
    "pnl_stats = pd.DataFrame({\n",
    "    \"Firm_A\": pd.Series(pnlA).describe(),\n",
    "    \"Firm_B\": pd.Series(pnlB).describe()\n",
    "}).T.round(2)\n",
    "display(pnl_stats.style.set_caption(\"PnL Distribution Statistics ($ Millions)\"))\n",
    "\n",
    "# PnL distributions plot\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.kdeplot(pnlA, label=\"Firm_A\", fill=True)\n",
    "sns.kdeplot(pnlB, label=\"Firm_B\", fill=True)\n",
    "cutA,cutB = np.percentile(pnlA,1), np.percentile(pnlB,1)\n",
    "plt.axvline(cutA, color='blue', linestyle='--', label=\"Firm_A 1% Tail\")\n",
    "plt.axvline(cutB, color='orange', linestyle='--', label=\"Firm_B 1% Tail\")\n",
    "plt.title(\"PnL Distributions with 1% Tail Thresholds\")\n",
    "plt.xlabel(\"PnL ($ Millions)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Identify material factors (~70% coverage)\n",
    "# Paper uses standardized shocks (+200/-200 bps) for materiality assessment\n",
    "material_factors = []\n",
    "material_coverages = []\n",
    "\n",
    "# Test both positive and negative standardized shocks as in the paper\n",
    "for sign in [-1, 1]:\n",
    "    standardized_shock = sign * 0.02  # 200 bps\n",
    "    \n",
    "    # Calculate PnL impacts with standardized shock\n",
    "    standardized_impacts = {}\n",
    "    for factor in all_names:\n",
    "        idx = all_names.index(factor)\n",
    "        # Create shock vector with only this factor shocked\n",
    "        shock_vector = np.zeros(len(all_names))\n",
    "        shock_vector[idx] = standardized_shock\n",
    "        \n",
    "        # Calculate PnL impact\n",
    "        impact = pnl(\"Firm_A\", shock_vector) + pnl(\"Firm_B\", shock_vector)\n",
    "        standardized_impacts[factor] = abs(impact)\n",
    "    \n",
    "    # Sort by impact magnitude\n",
    "    sorted_impacts = sorted(standardized_impacts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Calculate cumulative coverage\n",
    "    total_impact = sum(standardized_impacts.values())\n",
    "    cum_impact = 0\n",
    "    factors = []\n",
    "    \n",
    "    for factor, impact in sorted_impacts:\n",
    "        cum_impact += impact\n",
    "        coverage = cum_impact / total_impact\n",
    "        factors.append(factor)\n",
    "        \n",
    "        if coverage >= 0.7 and len(factors) >= 2:  # At least 2 factors\n",
    "            break\n",
    "    \n",
    "    material_factors.extend(factors)\n",
    "    material_coverages.extend([cum_impact/total_impact] * len(factors))\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "material = []\n",
    "for factor in material_factors:\n",
    "    if factor not in seen:\n",
    "        seen.add(factor)\n",
    "        material.append(factor)\n",
    "\n",
    "# Get material indices\n",
    "mat_idx = [all_names.index(m) for m in material]\n",
    "\n",
    "mat_df = pd.DataFrame({\n",
    "    \"Material Factor\": material,\n",
    "    \"Cum. Coverage\": [material_coverages[material_factors.index(m)] for m in material]\n",
    "})\n",
    "display(mat_df.style.set_caption(\"Material Risk Factors (~70% DV01 Coverage)\"))\n",
    "\n",
    "# 7. Tail-loss scenarios\n",
    "tail = np.unique(np.concatenate([np.where(pnlA<=cutA)[0], np.where(pnlB<=cutB)[0]]))\n",
    "tail_mat = scenarios[tail][:,mat_idx]\n",
    "\n",
    "# 8. Cluster analysis for tail scenarios\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k,random_state=42).fit(tail_mat)\n",
    "clusters = km.labels_\n",
    "centers = km.cluster_centers_\n",
    "representatives = []\n",
    "for cl in range(k):\n",
    "    mem = np.where(clusters==cl)[0]\n",
    "    d = np.linalg.norm(tail_mat[mem]-centers[cl],axis=1)\n",
    "    representatives.append(tail[mem[np.argmin(d)]])\n",
    "\n",
    "print(f\"Representative scenarios: {representatives}\")\n",
    "\n",
    "# Annotate silhouette scores for k=2..5\n",
    "sil_scores = [silhouette_score(tail_mat, KMeans(n_clusters=i,random_state=42).fit_predict(tail_mat)) for i in range(2,6)]\n",
    "sil_df = pd.DataFrame({\"k\":range(2,6),\"silhouette\":sil_scores})\n",
    "display(sil_df.style.format({\"silhouette\":\"{:.3f}\"}).set_caption(\"Silhouette Scores by k\"))\n",
    "\n",
    "# 9. Plot optimal number of clusters (like paper)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(2,6), sil_scores, marker='o', color='black')\n",
    "plt.axvline(2, linestyle='--', color='grey')\n",
    "plt.title(\"Optimal Number of Clusters for Interest Rate Shocks\")\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Average silhouette width\")\n",
    "plt.xticks(range(2,6))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 10. Cluster scatter plot\n",
    "plt.figure(figsize=(6,5))\n",
    "palette = {0:\"tomato\",1:\"teal\"}\n",
    "for cl in range(k):\n",
    "    pts = tail_mat[clusters==cl]\n",
    "    plt.scatter(pts[:,0], pts[:,1], color=palette[cl], label=f\"Cluster {cl+1}\", alpha=0.6)\n",
    "plt.scatter(centers[:,0], centers[:,1], marker=\"^\", s=150, color=\"gold\", label=\"Centers\")\n",
    "plt.axhline(0,color='black'); plt.axvline(0,color='black')\n",
    "plt.xlabel(f\"{material[0]} Shock\")\n",
    "plt.ylabel(f\"{material[1]} Shock\")\n",
    "plt.title(\"Clusters of Tail-Loss Scenarios\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Other diagnostics and scenario tables ---\n",
    "for i, rep in enumerate(representatives, 1):\n",
    "    print(f\"\\nScenario {i}:\")\n",
    "    scenario_vals = pd.Series(scenarios[rep], index=all_names)\n",
    "    mat_vals = scenario_vals[material].round(5)\n",
    "    pnl_a_val = pnlA[rep]\n",
    "    pnl_b_val = pnlB[rep]\n",
    "    display(pd.DataFrame({\n",
    "        \"Material Factor Shock\": mat_vals\n",
    "    }))\n",
    "    print(f\"Firm_A PnL Impact: ${pnl_a_val:,.2f}M\")\n",
    "    print(f\"Firm_B PnL Impact: ${pnl_b_val:,.2f}M\")\n",
    "\n",
    "# Primary shocks over time\n",
    "plt.figure(figsize=(14, 6))\n",
    "for col in primary_shocks.columns:\n",
    "    plt.plot(primary_shocks.index, primary_shocks[col], lw=1.2, label=col)\n",
    "plt.title(\"Primary Factor 3-Month Shocks Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Shock\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Histogram secondary factor shocks (first 5)\n",
    "plt.figure(figsize=(10, 5))\n",
    "secondary_sim.iloc[:, :5].hist(bins=30, alpha=0.7, layout=(1,5), figsize=(15,3))\n",
    "plt.suptitle(\"Histogram of Example Secondary Factor Shocks\")\n",
    "plt.show()\n",
    "\n",
    "# Bar plot: Scenario PnLs vs Tail Percentiles\n",
    "plt.figure(figsize=(10,6))\n",
    "data_to_plot = {\n",
    "    'Scenario 1 Firm_A': pnlA[representatives[0]],\n",
    "    'Scenario 1 Firm_B': pnlB[representatives[0]],\n",
    "    'Scenario 2 Firm_A': pnlA[representatives[1]],\n",
    "    'Scenario 2 Firm_B': pnlB[representatives[1]],\n",
    "    'Firm_A 1% Tail': cutA,\n",
    "    'Firm_A Median': np.median(pnlA),\n",
    "    'Firm_A 99% Percentile': np.percentile(pnlA, 99),\n",
    "    'Firm_B 1% Tail': cutB,\n",
    "    'Firm_B Median': np.median(pnlB),\n",
    "    'Firm_B 99% Percentile': np.percentile(pnlB, 99)\n",
    "}\n",
    "plt.bar(data_to_plot.keys(), data_to_plot.values(), color=sns.color_palette(\"Set1\", 10))\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"PnL ($ Millions)\")\n",
    "plt.title(\"Scenario PnLs Compared to Firm Percentiles\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
